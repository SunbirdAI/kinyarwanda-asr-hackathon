{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3410c5-421b-4f6e-91c4-6a79e807e390",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q jiwer==3.1.0\n",
    "!pip install -q accelerate\n",
    "!pip install -q transformers \n",
    "!pip install -q soundfile\n",
    "!git clone https://github.com/SunbirdAI/salt.git\n",
    "!pip install -qr salt/requirements.txt\n",
    "!pip install -q peft\n",
    "!pip install -q evaluate\n",
    "!pip install -q silero_vad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d407595a-8008-496a-a349-a76797380cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Union, List, Dict, Any\n",
    "import string\n",
    "import os\n",
    "import json\n",
    "import datasets\n",
    "import numpy as np\n",
    "import yaml\n",
    "import evaluate\n",
    "import salt.dataset\n",
    "import salt.metrics\n",
    "import salt.constants\n",
    "from salt.utils import DataCollatorCTCWithPadding as dcwp\n",
    "import huggingface_hub\n",
    "import peft\n",
    "import pandas as pd\n",
    "import tqdm.notebook as tqdm\n",
    "import jiwer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8593b234-3873-48a8-8577-5d77bc12c670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_punctuation(text):\n",
    "    # Create a translation table to remove all punctuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "    \n",
    "def normalise(texts):\n",
    "    return [strip_punctuation(t.lower()) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d44c626-ae85-466c-b9e3-cb0313ceea64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0443e893b7464a35b1252fcc3043f34a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = {'pretrained_model': 'jq/whisper-large-v3-kin-track-b'}\n",
    "feature_extractor = transformers.WhisperFeatureExtractor.from_pretrained(\n",
    "    config['pretrained_model'])\n",
    "processor = transformers.WhisperProcessor.from_pretrained(\n",
    "    config['pretrained_model'],\n",
    "    language=None,\n",
    "    task=\"transcribe\")\n",
    "model = transformers.WhisperForConditionalGeneration.from_pretrained(\n",
    "    config['pretrained_model'],\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    "    # torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    ")\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5390175-9b93-4e31-a2a2-57e75db296d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.generation_config.forced_decoder_ids = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2646c50-e4ae-4f07-92ac-8b0efabf4b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from silero_vad import load_silero_vad, read_audio, get_speech_timestamps\n",
    "\n",
    "def trim_audio_with_silero_vad(audio: np.ndarray, sample_rate: int = 16000) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Trim non-speech segments from audio using Silero VAD.\n",
    "    \n",
    "    Args:\n",
    "        audio: Audio array (single channel, float32)\n",
    "        sample_rate: Sample rate (default 16000)\n",
    "    \n",
    "    Returns:\n",
    "        Trimmed audio array with only speech segments\n",
    "    \"\"\"\n",
    "    # Load Silero VAD model\n",
    "    model = load_silero_vad()\n",
    "    \n",
    "    # Ensure audio is float32 and in correct range\n",
    "    if audio.dtype != np.float32:\n",
    "        audio = audio.astype(np.float32)\n",
    "    \n",
    "    # Normalize audio to [-1, 1] range if needed\n",
    "    if np.max(np.abs(audio)) > 1.0:\n",
    "        audio = audio / np.max(np.abs(audio))\n",
    "    \n",
    "    # Convert to torch tensor\n",
    "    audio_tensor = torch.from_numpy(audio)\n",
    "    \n",
    "    # Get speech timestamps\n",
    "    speech_timestamps = get_speech_timestamps(\n",
    "        audio_tensor, \n",
    "        model,\n",
    "        sampling_rate=sample_rate,\n",
    "        threshold=0.5,  # Adjust sensitivity (0.1-0.9)\n",
    "        min_speech_duration_ms=250,  # Minimum speech segment length\n",
    "        min_silence_duration_ms=100,  # Minimum silence to split on\n",
    "        window_size_samples=1536,  # VAD window size\n",
    "        speech_pad_ms=30  # Padding around speech segments\n",
    "    )\n",
    "    \n",
    "    # Extract speech segments\n",
    "    if not speech_timestamps:\n",
    "        return np.array([])  # No speech detected\n",
    "    \n",
    "    # Collect all speech chunks\n",
    "    speech_chunks = []\n",
    "    for timestamp in speech_timestamps:\n",
    "        start_sample = timestamp['start']\n",
    "        end_sample = timestamp['end']\n",
    "        chunk = audio[start_sample:end_sample]\n",
    "        speech_chunks.append(chunk)\n",
    "    \n",
    "    # Concatenate all speech segments\n",
    "    trimmed_audio = np.concatenate(speech_chunks)\n",
    "    \n",
    "    return trimmed_audio\n",
    "\n",
    "def get_speech_segments(audio: np.ndarray, sample_rate: int = 16000) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Get speech segment timestamps without trimming.\n",
    "    \n",
    "    Args:\n",
    "        audio: Audio array (single channel, float32)\n",
    "        sample_rate: Sample rate (default 16000)\n",
    "    \n",
    "    Returns:\n",
    "        List of (start_sample, end_sample) tuples for speech segments\n",
    "    \"\"\"\n",
    "    # Load Silero VAD model\n",
    "    model = load_silero_vad()\n",
    "    \n",
    "    # Ensure audio is float32 and in correct range\n",
    "    if audio.dtype != np.float32:\n",
    "        audio = audio.astype(np.float32)\n",
    "    \n",
    "    # Normalize audio to [-1, 1] range if needed\n",
    "    if np.max(np.abs(audio)) > 1.0:\n",
    "        audio = audio / np.max(np.abs(audio))\n",
    "    \n",
    "    # Convert to torch tensor\n",
    "    audio_tensor = torch.from_numpy(audio)\n",
    "    \n",
    "    # Get speech timestamps\n",
    "    speech_timestamps = get_speech_timestamps(\n",
    "        audio_tensor, \n",
    "        model,\n",
    "        sampling_rate=sample_rate,\n",
    "        threshold=0.5,\n",
    "        min_speech_duration_ms=250,\n",
    "        min_silence_duration_ms=100,\n",
    "        window_size_samples=1536,\n",
    "        speech_pad_ms=30\n",
    "    )\n",
    "    \n",
    "    segments = [(ts['start'], ts['end']) for ts in speech_timestamps]\n",
    "    return segments\n",
    "\n",
    "def trim_audio_start_end(audio: np.ndarray, sample_rate: int = 16000, buffer_seconds: float = 0.5, minimum_seconds_removed: float = 3.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Trim silence from the beginning and end, keeping internal silences and a buffer.\n",
    "    \n",
    "    Args:\n",
    "        audio: Audio array (single channel, float32)\n",
    "        sample_rate: Sample rate (default 16000)\n",
    "        buffer_seconds: Minimum buffer to leave at start/end (default 0.5s)\n",
    "    \n",
    "    Returns:\n",
    "        Audio array with silence trimmed from start and end, but with buffer preserved\n",
    "    \"\"\"\n",
    "    segments = get_speech_segments(audio, sample_rate)\n",
    "    \n",
    "    if not segments:\n",
    "        return np.array([])  # No speech detected\n",
    "    \n",
    "    # Get overall start and end of speech\n",
    "    speech_start = segments[0][0]\n",
    "    speech_end = segments[-1][1]\n",
    "    \n",
    "    # Calculate buffer in samples\n",
    "    buffer_samples = int(buffer_seconds * sample_rate)\n",
    "    \n",
    "    # Apply buffer while staying within audio bounds\n",
    "    trim_start = max(0, speech_start - buffer_samples)\n",
    "    trim_end = min(len(audio), speech_end + buffer_samples)\n",
    "\n",
    "    seconds_removed = (len(audio) - (trim_end - trim_start)) / sample_rate\n",
    "\n",
    "    if seconds_removed > minimum_seconds_removed:\n",
    "        # Return audio with buffer\n",
    "        return audio[trim_start:trim_end], seconds_removed\n",
    "    else:\n",
    "        return audio, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69e5a098-c2f4-4bfc-a19b-b9c4788d0d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f53fb6041944127a35765d04dbbab03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19dd6e877d3740e69df9f855a92c2401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions on the entire test set, or do a small sample from dev_test to check metrics look OK.\n",
    "predict_full_test_set = False\n",
    "\n",
    "test_repo = 'jq/kinyarwanda-speech-hackathon'\n",
    "\n",
    "if predict_full_test_set:\n",
    "    test_ds = datasets.load_dataset(test_repo, split='test', num_proc=10)\n",
    "else:\n",
    "    test_ds = datasets.load_dataset(test_repo, split='dev_test[:300]')\n",
    "    \n",
    "test_ds = test_ds.cast_column(\"audio\", datasets.Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19945191-c72e-427f-95f4-3441f5e6a304",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIM_LEADING_TRAILING_SILENCE = False\n",
    "NUM_BEAMS = 5\n",
    "NORMALISE_VOLUME = False\n",
    "\n",
    "test_ids = []\n",
    "test_transcriptions = []\n",
    "test_labels = []\n",
    "\n",
    "pipe = transformers.pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    return_timestamps=True,\n",
    "    generate_kwargs={\n",
    "        \"language\": processor.tokenizer.decode(\n",
    "            salt.constants.SALT_LANGUAGE_TOKENS_WHISPER['kin']),\n",
    "        \"num_beams\": NUM_BEAMS,\n",
    "    },\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "for i in tqdm.tqdm(range(len(test_ds))):   \n",
    "    example = test_ds[i]\n",
    "\n",
    "    audio_array = example['audio']['array']\n",
    "    \n",
    "    if NORMALISE_VOLUME:\n",
    "        audio_array = audio_array / np.max(np.abs(audio_array))\n",
    "\n",
    "    if TRIM_LEADING_TRAILING_SILENCE:\n",
    "        audio_array, _ = trim_audio_start_end(audio_array)\n",
    "\n",
    "    if len(audio_array) / 16000 < 30.0:\n",
    "        input_features = processor(\n",
    "            audio_array, sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "        input_features = input_features.to('cuda')\n",
    "            \n",
    "        predicted_ids = model.generate(\n",
    "            input_features,\n",
    "            num_beams=5,\n",
    "            max_length=400,\n",
    "            language=processor.tokenizer.decode(salt.constants.SALT_LANGUAGE_TOKENS_WHISPER['kin']),\n",
    "        )\n",
    "        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    else:\n",
    "        result = pipe(audio_array)\n",
    "        transcription = result['text']\n",
    "\n",
    "    if not predict_full_test_set:\n",
    "        test_labels.append(example['text'])\n",
    "\n",
    "    test_transcriptions.append(transcription)\n",
    "    test_ids.append(example['id'])\n",
    "    \n",
    "if not predict_full_test_set:\n",
    "    total_wer = jiwer.wer(normalise(test_labels), normalise(test_transcriptions))\n",
    "    total_cer = jiwer.cer(normalise(test_labels), normalise(test_transcriptions))\n",
    "    score = 1 - (0.6 * total_cer + 0.4 * total_wer)\n",
    "    \n",
    "    print(f\"Word Error Rate (WER): {total_wer:.4f}\")\n",
    "    print(f\"Character Error Rate (CER): {total_cer:.4f}\")\n",
    "    print(f\"Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce34357-5e64-47dc-91b5-7b186339143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.json') as f:\n",
    "    test_metadata = json.load(f)\n",
    "\n",
    "test_keys = test_metadata.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa0fde2-a44a-42cb-824c-1e41ed246b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "for i, pred in zip(test_ids, test_transcriptions):\n",
    "    predictions[i] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d02cf27-f498-49ed-af50-6d3f6513846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def strip_punctuation(text):\n",
    "    # Create a translation table to remove all punctuation\n",
    "    punctuation = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    translator = str.maketrans('', '', punctuation)\n",
    "    return text.translate(translator)\n",
    "    \n",
    "with open(f'submission-{PART}.csv', \"w\", encoding=\"utf-8\") as f:\n",
    "    if PART == 1:\n",
    "        f.write('id,transcription\\n')\n",
    "    for k in test_keys:\n",
    "        pred = predictions.get(k)\n",
    "        if not pred:\n",
    "            print('No prediction for key ', k)\n",
    "            f.write(f\"{k},a\\n\")\n",
    "        else:\n",
    "            normalised_pred = strip_punctuation(pred.lower())\n",
    "            f.write(f\"{k},{normalised_pred}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c29127b-c965-4073-ab4d-90a160098296",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031726c4-0817-4c18-8f51-cb26641439ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6591d621-0f5f-420c-912f-24e513497ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
