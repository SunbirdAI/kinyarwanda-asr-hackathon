{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e43562-c0d6-4763-a54b-8637e5182956",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!git clone https://github.com/sunbirdai/salt.git\n",
    "!pip install -r salt/requirements.txt\n",
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deddce9-fddd-4644-abbd-2bc21b4d9e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:2048\n",
    "!export CUDA_LAUNCH_BLOCKING=0\n",
    "!export CUBLAS_WORKSPACE_CONFIG=:4096:8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f103cb-83cd-4a52-86ee-286390e2d998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import salt.constants\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import WhisperProcessor, pipeline\n",
    "from typing import Optional, Union, List\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05daba8-28fa-46fb-9098-5ecd9442eb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6620d9-be88-463e-90e6-cada631970ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable optimizations\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cuda.enable_flash_sdp(True)  # A100 flash attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0ed4ba-b058-404a-bc98-9bfe1361bbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"jq/whisper-large-v3-kin-track-b\"\n",
    "HF_TOKEN = \"\" #hf token or login into hugging face\n",
    "OUTPUT_FILE = \"/kin/whisper_transcriptions.csv\"\n",
    "DATASET_NAME = \"jq/kinyarwanda-speech-hackathon\"\n",
    "LANGUAGE = \"kinyarwanda\"    \n",
    "BATCH_SIZE = 100        \n",
    "CHECKPOINT_EVERY = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363ef8ad-6cd3-4890-9940-8bbbd4c1067e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Whisper:\n",
    "    LANGUAGE_CODES = {v.lower(): k for k, v in salt.constants.SALT_LANGUAGE_NAMES.items()}\n",
    "    LANGUAGE_ID_TOKENS = salt.constants.SALT_LANGUAGE_TOKENS_WHISPER.copy()\n",
    "    \n",
    "    DEFAULT_GENERATION_CONFIG = {\n",
    "        \"prompt_condition_type\": \"first-segment\",\n",
    "        \"condition_on_prev_tokens\": True,\n",
    "        \"repetition_penalty\": 1.1,\n",
    "        \"no_repeat_ngram_size\": 5,\n",
    "        \"num_beams\": 1,\n",
    "        \"max_length\": 448,\n",
    "        \"do_sample\": False,\n",
    "    }\n",
    "    \n",
    "    SAMPLE_RATE = 16000\n",
    "\n",
    "    def __init__(self, model_name: str, hf_token: str):\n",
    "        self.device = torch.device(\"cuda\")\n",
    "        dtype = torch.float16  \n",
    "        \n",
    "        print(\"Loading processor...\")\n",
    "        self.processor = WhisperProcessor.from_pretrained(\n",
    "            model_name, \n",
    "            token=hf_token, \n",
    "            language=None,\n",
    "            cache_dir=\"/tmp/whisper_cache\"\n",
    "        )\n",
    "\n",
    "        print(\"Loading model...\")\n",
    "        self.model = pipeline(\n",
    "            task=\"automatic-speech-recognition\",\n",
    "            model=model_name,\n",
    "            tokenizer=self.processor.tokenizer,\n",
    "            feature_extractor=self.processor.feature_extractor,\n",
    "            device=self.device,\n",
    "            torch_dtype=dtype,\n",
    "            token=hf_token,\n",
    "            model_kwargs={\n",
    "                \"attn_implementation\": \"sdpa\",\n",
    "                \"use_cache\": True,\n",
    "                \"low_cpu_mem_usage\": True,\n",
    "            },\n",
    "            chunk_length_s=30,\n",
    "            batch_size=BATCH_SIZE, \n",
    "        )\n",
    "        \n",
    "        # Try to compile for extra speed\n",
    "        try:\n",
    "            self.model.model = torch.compile(self.model.model, mode=\"max-autotune\")\n",
    "            print(\"âœ… Model compiled successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Model compilation not available: {e}\")\n",
    "\n",
    "    def get_language_code(self, language: str) -> str:\n",
    "        lang_key = language.strip().lower()\n",
    "        if lang_key not in self.LANGUAGE_CODES:\n",
    "            raise ValueError(f\"Language '{language}' is not supported.\")\n",
    "        iso_code = self.LANGUAGE_CODES[lang_key]\n",
    "        if iso_code not in self.LANGUAGE_ID_TOKENS:\n",
    "            raise ValueError(f\"No token mapping found for language code '{iso_code}'.\")\n",
    "        token = self.LANGUAGE_ID_TOKENS[iso_code]\n",
    "        decoded_lang = self.processor.tokenizer.decode(token)[2:-2]\n",
    "        return decoded_lang\n",
    "\n",
    "    def transcribe_batch(self, audio_arrays: List[np.ndarray], language: str) -> List[str]:\n",
    "        \"\"\"Batch transcription with updated autocast API\"\"\"\n",
    "        language_code = self.get_language_code(language)\n",
    "        \n",
    "        # Prepare batch input\n",
    "        batch_input = [{\"array\": audio_array, \"sampling_rate\": self.SAMPLE_RATE} \n",
    "                      for audio_array in audio_arrays]\n",
    "        \n",
    "        try:\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                results = self.model(\n",
    "                    batch_input,\n",
    "                    generate_kwargs={\n",
    "                        **self.DEFAULT_GENERATION_CONFIG,\n",
    "                        \"language\": language_code\n",
    "                    },\n",
    "                    return_timestamps=False\n",
    "                )\n",
    "            \n",
    "            if isinstance(results, list):\n",
    "                return [result['text'] for result in results]\n",
    "            else:\n",
    "                return [results['text']]\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Batch error: {e}\")\n",
    "            return [f\"[ERROR] {e}\"] * len(audio_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e70114-bb69-40b3-8e80-f84606a7c018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "whisper_model = Whisper(MODEL_NAME, HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5059bad-31cb-4e46-9a52-ab261ed979cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# streaming  dataset \n",
    "print(\"ðŸ“ Loading dataset...\")\n",
    "dataset = load_dataset(DATASET_NAME, split=\"train\", token=HF_TOKEN, streaming=True)\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2669602d-9e5d-4dce-b781-30ab6e0511b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_dataset():\n",
    "    \"\"\"\n",
    "    single-GPU transcription\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Starting transcription...\")\n",
    "    \n",
    "    # Resume support\n",
    "    if os.path.exists(OUTPUT_FILE):\n",
    "        existing_df = pd.read_csv(OUTPUT_FILE)\n",
    "        completed_ids = set(existing_df[\"id\"])\n",
    "        print(f\"ðŸ“‹ Resuming from {len(completed_ids)} completed samples\")\n",
    "    else:\n",
    "        completed_ids = set()\n",
    "    \n",
    "    # Processing\n",
    "    results = []\n",
    "    batch_data = []\n",
    "    processed_count = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"Starting batch processing...\")\n",
    "\n",
    "    for sample in tqdm(dataset.skip(len(completed_ids)), desc=\"Processing\"):\n",
    "        if sample[\"id\"] in completed_ids:\n",
    "            continue\n",
    "            \n",
    "        batch_data.append(sample)\n",
    "        \n",
    "        # Process when batch is full\n",
    "        if len(batch_data) >= BATCH_SIZE:\n",
    "            audio_arrays = [s[\"audio\"][\"array\"] for s in batch_data]\n",
    "            \n",
    "            try:\n",
    "                transcriptions = whisper_model.transcribe_batch(audio_arrays, LANGUAGE)\n",
    "                \n",
    "                for sample, transcription in zip(batch_data, transcriptions):\n",
    "                    results.append({\n",
    "                        \"id\": sample[\"id\"],\n",
    "                        \"language\": LANGUAGE.lower(),\n",
    "                        \"original_text\": sample[\"text\"],\n",
    "                        \"transcription\": transcription\n",
    "                    })\n",
    "                \n",
    "                processed_count += len(batch_data)\n",
    "                \n",
    "                # Show progress\n",
    "                elapsed = time.time() - start_time\n",
    "                rate = processed_count / elapsed if elapsed > 0 else 0\n",
    "                eta = (300000 - processed_count) / rate / 3600 if rate > 0 else 0\n",
    "                print(f\"âš¡ Processed: {processed_count} | Rate: {rate:.1f}/sec | ETA: {eta:.1f}h\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Batch processing error: {e}\")\n",
    "                # Add error entries\n",
    "                for sample in batch_data:\n",
    "                    results.append({\n",
    "                        \"id\": sample[\"id\"],\n",
    "                        \"language\": LANGUAGE.lower(),\n",
    "                        \"original_text\": sample[\"text\"],\n",
    "                        \"transcription\": f\"[ERROR] {e}\"\n",
    "                    })\n",
    "                processed_count += len(batch_data)\n",
    "            \n",
    "            batch_data = []\n",
    "            \n",
    "            # Memory cleanup\n",
    "            if processed_count % 100 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "            \n",
    "            # Checkpoint\n",
    "            if len(results) >= CHECKPOINT_EVERY:\n",
    "                print(f\"Checkpointing at {processed_count} samples...\")\n",
    "                df = pd.DataFrame(results)\n",
    "                df.to_csv(\n",
    "                    OUTPUT_FILE,\n",
    "                    mode=\"a\",\n",
    "                    index=False,\n",
    "                    header=not os.path.exists(OUTPUT_FILE)\n",
    "                )\n",
    "                results = []\n",
    "    \n",
    "    # Process remaining batch\n",
    "    if batch_data:\n",
    "        audio_arrays = [s[\"audio\"][\"array\"] for s in batch_data]\n",
    "        transcriptions = whisper_model.transcribe_batch(audio_arrays, LANGUAGE)\n",
    "        \n",
    "        for sample, transcription in zip(batch_data, transcriptions):\n",
    "            results.append({\n",
    "                \"id\": sample[\"id\"],\n",
    "                \"language\": LANGUAGE.lower(),\n",
    "                \"original_text\": sample[\"text\"],\n",
    "                \"transcription\": transcription\n",
    "            })\n",
    "    \n",
    "    # Save final results\n",
    "    if results:\n",
    "        df = pd.DataFrame(results)\n",
    "        df.to_csv(\n",
    "            OUTPUT_FILE,\n",
    "            mode=\"a\",\n",
    "            index=False,\n",
    "            header=not os.path.exists(OUTPUT_FILE)\n",
    "        )\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Transcription complete!\")\n",
    "    print(f\"Total time: {total_time/3600:.2f} hours\")\n",
    "    print(f\"Average rate: {processed_count/(total_time/60):.1f} samples/minute\")\n",
    "    print(f\" Results saved to: {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810afe9b-0d3b-4398-b57e-d008636899dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Run the optimized transcription\n",
    "    transcribe_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454a30f0-2f0f-4e6d-b6bb-54d80debc9cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
